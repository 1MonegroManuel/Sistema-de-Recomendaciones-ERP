# reco_service_stream.py
# Servicio en tiempo real: consume ventas, genera recomendaciones y publica a Kafka

import json, pickle, time, signal, sys
from pathlib import Path
from typing import List, Dict
from kafka import KafkaConsumer, KafkaProducer
from pymongo import MongoClient
from datetime import datetime
sys.path.append(str(Path(__file__).parent.parent))
from config.config import MONGODB_CONNECTION_STRING, MONGODB_DATABASE, KAFKA_BOOTSTRAP_SERVERS

# Variable global para manejar se√±ales de interrupci√≥n
shutdown_flag = False

def signal_handler(sig, frame):
    """Maneja Ctrl+C correctamente"""
    global shutdown_flag
    print("\n\nüõë Se√±al de interrupci√≥n recibida. Deteniendo...")
    shutdown_flag = True
    sys.exit(0)

# Registrar el manejador de se√±ales
signal.signal(signal.SIGINT, signal_handler)

BROKER     = KAFKA_BOOTSTRAP_SERVERS  # ‚úÖ Usa config.py
TOPIC_SALES= "erp.sales"             # cabezal de venta (ya lo tienes en pr√°ctica 3)
TOPIC_RECO = "erp.recommendations"   # nuevo t√≥pico para recomendaciones

MONGO_URI  = MONGODB_CONNECTION_STRING  # ‚úÖ Usa MongoDB Atlas
DB_NAME    = MONGODB_DATABASE          # ‚úÖ Usa erp_database (donde est√°n tus datos)
DET_COLL   = "sales_items"             # ‚úÖ Colecci√≥n correcta
RECO_COLL  = "recommendations"         # ‚úÖ Colecci√≥n para guardar recomendaciones

MODEL_PATH = str(Path(__file__).parent.parent / "data" / "models" / "item_sim.pkl")
TOP_N      = 5

# ------- util -------
def load_model(path=MODEL_PATH):
    with open(path, "rb") as f:
        data = pickle.load(f)
    return data["item_sim"], data["item_count"]

def get_basket_products(db, sale_id: int) -> List[int]:
    rows = list(db[DET_COLL].find({"sale_id": int(sale_id)}, {"product_id":1}))
    return [int(r["product_id"]) for r in rows if "product_id" in r]

def enrich_recommendations_with_product_names(db, recommendations: List[Dict], products_cache: Dict = None) -> List[Dict]:
    """ Enriquece las recomendaciones con nombres, categor√≠as y m√©tricas de ML en porcentajes """
    if not recommendations:
        return []
    
    # Obtener IDs de productos √∫nicos
    product_ids = [r.get("product_id") for r in recommendations if r.get("product_id")]
    
    if not product_ids:
        return recommendations
    
    # Calcular m√©tricas de ML (normalizar relevancias a porcentajes)
    relevancias = [r.get("relevancia", r.get("score", 0.0)) for r in recommendations]  # Compatibilidad: acepta "score" o "relevancia"
    max_relevancia = max(relevancias) if relevancias else 1.0
    min_relevancia = min(relevancias) if relevancias else 0.0
    relevancia_range = max_relevancia - min_relevancia if max_relevancia > min_relevancia else 1.0
    
    # Cargar productos que no est√©n en el cache
    products_to_fetch = []
    if products_cache is None:
        products_cache = {}
    
    for pid in product_ids:
        if pid not in products_cache:
            products_to_fetch.append(pid)
    
    # Consultar MongoDB solo para productos que no est√°n en cache
    if products_to_fetch:
        fetched = db.products.find(
            {"product_id": {"$in": products_to_fetch}},
            {"product_id": 1, "name": 1, "category": 1}
        )
        for p in fetched:
            pid = int(p.get("product_id", 0))
            products_cache[pid] = {
                "name": p.get("name", f"Producto #{pid}"),
                "category": p.get("category", "Sin categor√≠a")
            }
    
    # Enriquecer recomendaciones con nombres y m√©tricas de ML
    enriched = []
    total_recommendations = len(recommendations)
    
    for idx, r in enumerate(recommendations):
        pid = r.get("product_id")
        relevancia = r.get("relevancia", r.get("score", 0.0))  # Compatibilidad: acepta "score" o "relevancia"
        product_info = products_cache.get(pid, {})
        
        # Calcular m√©tricas de ML en porcentajes
        # Relevancia normalizada a porcentaje (0-100%)
        relevancia_percentage = ((relevancia - min_relevancia) / relevancia_range * 100.0) if relevancia_range > 0 else 0.0
        
        # Confianza basada en la relevancia (m√°s alto = m√°s confianza)
        confidence_percentage = min(100.0, relevancia_percentage * 1.2)  # Ajustar para que max sea 100%
        
        # Ranking normalizado (1 = mejor, porcentaje invertido)
        ranking = idx + 1
        ranking_percentage = ((total_recommendations - ranking + 1) / total_recommendations * 100.0) if total_recommendations > 0 else 0.0
        
        enriched_rec = {
            "product_id": pid,
            "product_name": product_info.get("name", f"Producto #{pid}"),
            "category": product_info.get("category", "Sin categor√≠a"),
            "relevancia": relevancia,  # Relevancia original (compatibilidad: mantener "score" tambi√©n)
            "score": relevancia,  # Mantener para compatibilidad hacia atr√°s
            "relevancia_percentage": round(relevancia_percentage, 2),  # Relevancia en porcentaje
            "score_percentage": round(relevancia_percentage, 2),  # Compatibilidad: mantener tambi√©n "score_percentage"
            "confidence_percentage": round(confidence_percentage, 2),  # Confianza en porcentaje
            "ranking": ranking,  # Ranking (1 = mejor)
            "ranking_percentage": round(ranking_percentage, 2)  # Ranking en porcentaje
        }
        enriched.append(enriched_rec)
    
    return enriched

def recommend_for_basket(item_sim: Dict[int, List], basket: List[int], k=TOP_N) -> List[Dict]:
    basket_uniq = set(basket)
    relevancias = {}  # producto -> relevancia acumulada
    for pid in basket_uniq:
        neighbors = item_sim.get(pid, [])
        for nb, sim in neighbors:
            if nb in basket_uniq: 
                continue  # no recomendar algo ya comprado
            relevancias[nb] = relevancias.get(nb, 0.0) + sim
    # top-k
    ranked = sorted(relevancias.items(), key=lambda x: x[1], reverse=True)[:k]
    return [{"product_id": int(p), "relevancia": float(s), "score": float(s)} for p, s in ranked]  # Mantener "score" para compatibilidad

def main():
    print("Cargando modelo‚Ä¶")
    try:
        item_sim, item_count = load_model()
        print(f"‚úÖ Modelo cargado. Items con vecinos: {len(item_sim)}")
    except FileNotFoundError:
        print(f"‚ùå Error: No se encuentra el modelo en {MODEL_PATH}")
        print("   Ejecuta primero: python model_train_reco.py")
        return
    except Exception as e:
        print(f"‚ùå Error cargando modelo: {e}")
        return

    print(f"Conectando a MongoDB Atlas ({DB_NAME})...")
    try:
        client = MongoClient(MONGO_URI)
        client.admin.command('ping')  # Verificar conexi√≥n
        db = client[DB_NAME]
        print(f"‚úÖ Conectado a MongoDB Atlas | Base de datos: {DB_NAME}")
    except Exception as e:
        print(f"‚ùå Error conectando a MongoDB Atlas: {e}")
        print(f"   Verifica tu conexi√≥n a internet y las credenciales en config.py")
        return
    
    # Cache de productos para enriquecer recomendaciones (evita consultas repetidas)
    products_cache = {}
    print(f"‚úÖ Cache de productos inicializado (se cargar√°n productos bajo demanda)")

    # Usar un group_id √∫nico para leer todos los mensajes disponibles
    group_id = f"reco-service-{int(time.time())}"
    
    print(f"üîå Configurando consumer Kafka...")
    print(f"   - T√≥pico: {TOPIC_SALES}")
    print(f"   - Broker: {BROKER}")
    print(f"   - Group ID: {group_id}")
    
    consumer = KafkaConsumer(
        TOPIC_SALES,
        bootstrap_servers=BROKER,
        value_deserializer=lambda v: json.loads(v.decode("utf-8")) if v else None,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        group_id=group_id,
        consumer_timeout_ms=5000,
        max_poll_records=20  # ‚úÖ Procesar solo 20 mensajes a la vez para no bloquearse
    )
    
    # Esperar a que el consumer se suscriba
    print(f"‚è≥ Suscribi√©ndose al t√≥pico...")
    time.sleep(2)
    
    print(f"üîå Configurando producer Kafka...")
    print(f"   - T√≥pico destino: {TOPIC_RECO}")
    print(f"   - Broker: {BROKER}")
    
    producer = KafkaProducer(
        bootstrap_servers=BROKER,
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8"),
        acks="0",  # ‚úÖ Cambiar a "0" para que no espere confirmaci√≥n (m√°s r√°pido)
        linger_ms=50,  # Esperar hasta 50ms para agrupar mensajes
        batch_size=16384,  # 16KB de batch (m√°s peque√±o para evitar bloqueos)
        compression_type=None,  # ‚úÖ Sin compresi√≥n para reducir latencia
        request_timeout_ms=30000,  # Timeout de 30 segundos para requests
        max_in_flight_requests_per_connection=5,  # Permitir m√∫ltiples requests en vuelo
        retries=0,  # ‚úÖ Sin reintentos para evitar acumulaci√≥n
        max_block_ms=10000,  # ‚úÖ Esperar hasta 10 segundos antes de fallar al enviar
        buffer_memory=67108864  # ‚úÖ 64MB de buffer (por defecto)
    )
    
    print(f"‚úÖ Producer Kafka creado correctamente")

    print(f"‚úÖ Conectado a Kafka | Broker: {BROKER}")
    print(f"üì• Esperando ventas en '{TOPIC_SALES}' ‚Ä¶")
    print(f"üì§ Publicando recomendaciones en '{TOPIC_RECO}'")
    print(f"üí° Leyendo TODOS los mensajes disponibles (desde el principio)")
    print(f"{'='*60}")
    
    msg_count = 0
    last_heartbeat = time.time()
    last_flush_time = time.time()
    FLUSH_INTERVAL = 5.0  # Hacer flush cada 5 segundos en lugar de despu√©s de cada batch
    try:
        while not shutdown_flag:
            polled = consumer.poll(timeout_ms=1000)
            if not polled:
                # No hay mensajes, mostrar heartbeat cada 15 segundos
                current_time = time.time()
                if current_time - last_heartbeat >= 15:
                    timestamp = datetime.now().strftime("%H:%M:%S")
                    print(f"üíì [{timestamp}] Esperando ventas... (procesadas: {msg_count})")
                    last_heartbeat = current_time
                continue
            
            # Mensajes recibidos
            total_msgs = sum(len(recs) for recs in polled.values())
            print(f"üì® Recibido lote de {total_msgs} mensaje(s)")
                
            for tp, recs in polled.items():
                batch_size = len(recs)
                print(f"   üì¶ Procesando {batch_size} mensaje(s) de partici√≥n {tp.partition}")
                processed_in_batch = 0
                errors_in_batch = 0
                for idx, rec in enumerate(recs, 1):
                    try:
                        msg_count += 1
                        sale = rec.value
                        
                        # Mostrar progreso cada 50 mensajes dentro del batch
                        if idx % 50 == 0 or idx == batch_size:
                            print(f"      ‚è≥ Progreso: {idx}/{batch_size} mensajes procesados en este batch...")
                        
                        if sale is None:
                            print(f"‚ö†Ô∏è  [{msg_count}] Mensaje None recibido")
                            continue
                            
                        if not isinstance(sale, dict):
                            print(f"‚ö†Ô∏è  [{msg_count}] Mensaje no es un dict: {type(sale)}")
                            continue
                        
                        # se espera un 'sale_id' en el mensaje de erp.sales
                        sale_id = sale.get("sale_id") or sale.get("id")
                        if not sale_id:
                            print(f"‚ö†Ô∏è  [{msg_count}] Mensaje sin sale_id: {list(sale.keys())[:5]}...")
                            continue

                        sale_id = int(sale_id)
                        basket = get_basket_products(db, sale_id)
                        
                        if not basket:
                            # si todav√≠a no est√°n los items, intenta m√°s tarde (puede venir desfasado)
                            time.sleep(0.3)
                            basket = get_basket_products(db, sale_id)
                            if not basket:
                                print(f"‚ö†Ô∏è  [{msg_count}] No se encontraron productos para sale_id={sale_id}")
                                continue

                        reco = recommend_for_basket(item_sim, basket, k=TOP_N)
                        
                        # ‚úÖ Enriquecer recomendaciones con nombres de productos y m√©tricas de ML
                        reco_enriched = enrich_recommendations_with_product_names(db, reco, products_cache)
                        
                        # ‚úÖ Obtener customer_id del mensaje de venta o de la base de datos
                        customer_id = sale.get("customer_id")
                        if not customer_id:
                            # Intentar obtener desde la base de datos
                            sale_doc = db.sales.find_one({"sale_id": sale_id}, {"customer_id": 1})
                            if sale_doc:
                                customer_id = sale_doc.get("customer_id")
                        
                        payload = {
                            "sale_id": sale_id,
                            "customer_id": customer_id,  # ‚úÖ A√±adir customer_id al payload
                            "timestamp": sale.get("sale_datetime") or sale.get("fecha_venta"),
                            "basket_products": list(map(int, basket)),
                            "recommendations": reco_enriched,  # ‚úÖ Ya incluye nombres y categor√≠as
                            "algo": "itemcf_cosine_v1",
                            "created_at": datetime.now().isoformat()  # Agregar timestamp de creaci√≥n
                        }
                        
                        # ‚úÖ GUARDAR EN MONGODB (respaldo/alternativa)
                        try:
                            db[RECO_COLL].update_one(
                                {"sale_id": sale_id},
                                {"$set": payload},
                                upsert=True
                            )
                            processed_in_batch += 1
                        except Exception as e:
                            print(f"‚ö†Ô∏è  [{msg_count}] Error guardando en MongoDB: {e}")
                        
                        # Enviar a Kafka tambi√©n (intento, pero MongoDB es el respaldo)
                        try:
                            future = producer.send(TOPIC_RECO, payload)
                            # Log solo para los primeros mensajes o cada 50 para confirmar env√≠o
                            if processed_in_batch <= 3 or processed_in_batch % 50 == 0:
                                timestamp = datetime.now().strftime("%H:%M:%S")
                                print(f"üì§ [{timestamp}] Enviando recomendaci√≥n para sale_id={sale_id} a Kafka y MongoDB (total procesadas: {processed_in_batch})")
                        except Exception as e:
                            # Error en Kafka no es cr√≠tico si ya guardamos en MongoDB
                            if processed_in_batch <= 5:
                                print(f"‚ö†Ô∏è  [{msg_count}] Error enviando a Kafka (pero guardado en MongoDB): {e}")
                            continue
                        last_heartbeat = time.time()
                        
                        # Solo mostrar los primeros 3 mensajes y luego cada 50
                        if processed_in_batch <= 3 or processed_in_batch % 50 == 0:
                            timestamp = datetime.now().strftime("%H:%M:%S")
                            print(f"‚úÖ [{timestamp}] #{msg_count} sale_id={sale_id} | basket={basket[:5]}‚Ä¶ | reco={[r['product_id'] for r in reco[:3]]}")
                    except Exception as e:
                        errors_in_batch += 1
                        if errors_in_batch <= 5:  # Solo mostrar los primeros 5 errores
                            print(f"‚ùå [{msg_count}] Error procesando mensaje: {e}")
                            import traceback
                            traceback.print_exc()
                        continue
                
                # Flush al final del batch - IMPORTANTE para asegurar env√≠o
                # Con acks="0" esto deber√≠a ser r√°pido. El flush asegura que los mensajes
                # en el buffer se env√≠en al broker de Kafka.
                if processed_in_batch > 0:  # Solo hacer flush si hay mensajes procesados
                    flush_start = time.time()
                    try:
                        producer.flush(timeout=10.0)  # Timeout de 10 segundos
                        flush_time = time.time() - flush_start
                        last_flush_time = time.time()
                        if flush_time > 2.0:  # Si tarda m√°s de 2 segundos, advertir
                            print(f"   ‚ö†Ô∏è  Flush tard√≥ {flush_time:.2f} segundos")
                    except Exception as e:
                        # Si el flush falla, es cr√≠tico - los mensajes no se enviaron
                        print(f"   ‚ùå Error cr√≠tico en flush: {type(e).__name__}: {e}")
                        # Intentar un √∫ltimo flush con timeout m√°s corto
                        try:
                            producer.flush(timeout=1.0)
                        except:
                            print(f"   ‚ùå Flush fall√≥ completamente. Los mensajes pueden no haberse enviado.")
                
                # Resumen del batch procesado
                timestamp = datetime.now().strftime("%H:%M:%S")
                print(f"   ‚úÖ [{timestamp}] Batch completado: {processed_in_batch} exitosos, {errors_in_batch} errores de {batch_size} mensajes")
            # loop
    except KeyboardInterrupt:
        print(f"\n{'='*60}")
        print(f"üõë Interrumpido por usuario.")
        print(f"üìä Total de mensajes procesados: {msg_count}")
    except Exception as e:
        print(f"\n‚ùå Error inesperado: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("üîå Cerrando conexiones...")
        producer.flush()
        producer.close()
        consumer.close()
        print("‚úÖ Servicio detenido correctamente.")

if __name__ == "__main__":
    main()

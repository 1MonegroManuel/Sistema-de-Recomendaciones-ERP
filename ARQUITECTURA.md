# ğŸ—ï¸ Arquitectura del Sistema de Recomendaciones ERP

## ğŸ“Š VisiÃ³n General

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FLUJO PRINCIPAL (Resumen)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fuente CSV â”€â–¶ Productor Kafka â”€â–¶ MongoDB â”€â–¶ ML â”€â–¶ Dashboards â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Esta arquitectura prioriza el pipeline de recomendaciones en tiempo real. Todo gira alrededor de ocho componentes clave que se mantienen sincronizados mediante Kafka, MongoDB y Streamlit.

## ğŸ”‘ Componentes Esenciales

- `docker-compose.yml`: levanta Kafka, Kafdrop y MongoDB para todo el ecosistema.
- `erp_simulator_producer.py`: carga los CSV normalizados y envÃ­a los mensajes al clÃºster de Kafka en el orden correcto (productos primero, despuÃ©s clientes, ventas y lÃ­neas).
- `erp_kafka_to_mongo.py`: consume los tÃ³picos `erp.*` y hace upsert en las colecciones de MongoDB (`products`, `customers`, `sales`, `sales_items`).
- `model_train_reco.py`: entrena el modelo item-based (co-ocurrencia + coseno) y genera `models/item_sim.pkl` con las relevancias entre productos.
- `reco_service_stream.py`: escucha ventas desde `erp.sales`, recolecta la cesta en MongoDB, calcula recomendaciones con el modelo, enriquece con nombres/porcentajes y publica la carga en Kafka y MongoDB (`recommendations`).
- `print_recommendations_consumer.py`: dashboard Streamlit que consume el tÃ³pico `erp.recommendations` (o MongoDB de respaldo) y muestra cada venta con sus compras y recomendaciones enriquecidas.
- `mongo_data_processor_completo.py`: genera resÃºmenes agregados (ventas diarias, ranking por tienda, top productos, clientes, categorÃ­as) para los dashboards histÃ³ricos.
- `erp_dashboard.py` y `erp_dashboard_dinamico.py`: dashboards Streamlit para visualizaciÃ³n histÃ³rica y monitoreo en tiempo real respectivamente.

## ğŸ”„ Flujo de Datos Detallado

```
CSV (csv_out/*.csv)
    â”‚
    â–¼
erp_simulator_producer.py â”€â”€â–º Kafka tÃ³picos erp.* â”€â”€â–º erp_kafka_to_mongo.py â”€â”€â–º MongoDB Atlas
                                                                              â”‚
                                                                              â”œâ”€â–º model_train_reco.py (entrenamiento offline)
                                                                              â”‚       â””â”€â–º models/item_sim.pkl
                                                                              â”‚
                                                                              â””â”€â–º reco_service_stream.py (online)
                                                                                      â”œâ”€â–º Kafka erp.recommendations
                                                                                      â””â”€â–º MongoDB recommendations

Kafka / MongoDB â”€â”€â–º print_recommendations_consumer.py (dashboard en vivo)
MongoDB           â”€â”€â–º mongo_data_processor_completo.py â”€â”€â–º colecciones agregadas â”€â”€â–º erp_dashboard*.py
```

## âš™ï¸ Roles de los Servicios

- **Kafka (`docker-compose.yml`)**: maneja el streaming de eventos `erp.products`, `erp.customers`, `erp.sales`, `erp.sales_items` y `erp.recommendations` sobre `localhost:9094`.
- **MongoDB Atlas**: almacÃ©n central de datos crudos y enriquecidos; permite consultas rÃ¡pidas para dashboards y para el servicio de recomendaciones.
- **Streamlit** (`print_recommendations_consumer.py`, `erp_dashboard.py`, `erp_dashboard_dinamico.py`): capa de presentaciÃ³n para usuarios finales (puertos 8501 y 8502).

## ğŸ§  Motor de Recomendaciones

1. `model_train_reco.py` calcula la similitud item-item ponderada por recencia y genera un diccionario de relevancias.
2. `reco_service_stream.py` combina el modelo con las cestas reales:
   - Recupera `sales_items` desde MongoDB.
   - Obtiene las mejores coincidencias (`relevancia`) del modelo.
   - Enriquecer con nombres, categorÃ­as y porcentajes (relevancia, confianza, ranking).
   - Guarda y publica el resultado para consumo inmediato.

## ğŸ–¥ï¸ Dashboards

- `print_recommendations_consumer.py`: vista detallada por venta (cliente, tienda, mÃ©todo de pago, productos comprados, recomendaciones y mÃ©tricas de ML).
- `erp_dashboard.py`: anÃ¡lisis histÃ³rico generado a partir de las colecciones agregadas (`daily_sales_summary`, `store_analysis`, etc.).
- `erp_dashboard_dinamico.py`: monitoreo en vivo de ventas utilizando consultas directas a `sales` y filtros en tiempo real.

## ğŸ”§ OperaciÃ³n BÃ¡sica

1. Levantar infraestructura: `docker-compose up -d` (Kafka, MongoDB, Kafdrop, etc.).
2. Procesar CSV reales si es necesario y ejecutar `erp_simulator_producer.py` para poblar Kafka.
3. Ejecutar `erp_kafka_to_mongo.py` para persistir en MongoDB.
4. Entrenar el modelo con `model_train_reco.py` (cuando se ingrese nuevo histÃ³rico significativo).
5. Iniciar `reco_service_stream.py` para generar recomendaciones en tiempo real.
6. Abrir dashboards Streamlit segÃºn necesidad:
   - `print_recommendations_consumer.py` (tiempo real con recomendaciones).
   - `erp_dashboard.py` / `erp_dashboard_dinamico.py` (histÃ³rico y live business).
7. Ejecutar `mongo_data_processor_completo.py` periÃ³dicamente para refrescar agregados.

## ğŸŒ Puertos y Conexiones

- Kafka (broker externo): `127.0.0.1:9094`
- Kafdrop (verificaciÃ³n tÃ³picos): `http://localhost:9000`
- MongoDB (Docker local o Atlas): `mongodb://admin:admin123@localhost:27017` (o string Atlas de `config.py`)
- Dashboards Streamlit: `http://localhost:8501` y `http://localhost:8502`

---

Arquitectura enfocada en recomendaciones en tiempo real, mantenida Ãºnicamente con los mÃ³dulos crÃ­ticos del proyecto.

# ğŸ¢ Sistema ERP de Big Data - Arquitectura de Recomendaciones Comerciales

## ğŸ“‹ Objetivo

Desarrollar un sistema comercial integral de recomendaciones basado en arquitectura big data que procese datos masivos de un ERP y genere insights comerciales mediante dashboards interactivos.

## ğŸ¯ Objetivos EspecÃ­ficos

### âœ… 1. Fuente de Datos - Sistema ERP
- **Productos**: CatÃ¡logo completo con categorÃ­as, precios y stock
- **Clientes**: Base de datos de clientes con segmentaciÃ³n
- **Ventas**: Transacciones comerciales con detalles de items

### âœ… 2. Procesador/Productor de Datos
- Procesamiento de datos reales del dataset Superstore (9,994 registros)
- ConversiÃ³n de formato denormalizado a estructura normalizada (products, customers, sales, sales_items)
- EnvÃ­o continuo de datos al Data Ingestor

### âœ… 3. Apache Kafka - Data Ingestor
- Procesamiento de streams de datos en tiempo real
- Particionamiento y distribuciÃ³n de mensajes
- GarantÃ­a de entrega y orden de mensajes

### âœ… 4. Almacenamiento - MongoDB Atlas
- Data Lake para datos raw (ventas, clientes, productos)
- Data Warehouse con datos procesados y agregados
- Colecciones especializadas para anÃ¡lisis

### âœ… 5. Procesamiento de Datos
- Agregaciones por dÃ­a, sucursal, producto y cliente
- AnÃ¡lisis estadÃ­stico y correlaciones
- GeneraciÃ³n de mÃ©tricas de negocio

### âœ… 6. Motor de Recomendaciones + VisualizaciÃ³n
- Entrenamiento del modelo item-based (co-ocurrencia + coseno)
- Servicio en tiempo real que publica recomendaciones enriquecidas
- Dashboards Streamlit (recomendaciones, histÃ³rico y dinÃ¡mico)

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FUENTE DE     â”‚    â”‚   DATA INGESTOR â”‚    â”‚   STORAGE &     â”‚
â”‚   DATOS REALES  â”‚â”€â”€â”€â–¶â”‚   APACHE KAFKA  â”‚â”€â”€â”€â–¶â”‚   PROCESSING    â”‚
â”‚  (Superstore)   â”‚    â”‚                 â”‚    â”‚   MONGODB ATLAS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â–¼
         â”‚                       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚              â”‚   DASHBOARDS    â”‚
         â”‚                       â”‚              â”‚   STREAMLIT     â”‚
         â”‚                       â”‚              â”‚   (EstÃ¡tico +   â”‚
         â”‚                       â”‚              â”‚    DinÃ¡mico)    â”‚
         â”‚                       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GENERADOR     â”‚    â”‚   CONSUMIDOR    â”‚
â”‚   CSV MASIVOS   â”‚    â”‚   KAFKAâ†’MONGO   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Componentes del Sistema

### ğŸ”§ Archivos Principales

| Archivo | FunciÃ³n | DescripciÃ³n |
|---------|---------|-------------|
| `process_superstore_csv.py` | Procesador CSV | Convierte datos reales del CSV Superstore a formato ERP |
| `erp_simulator_producer.py` | Productor Kafka | EnvÃ­a datos CSV a Kafka (productos primero para asegurar nombres en MongoDB) |
| `erp_kafka_to_mongo.py` | Consumidor Kafka | Recibe datos de Kafka y los guarda en MongoDB |
| `model_train_reco.py` | Entrenamiento ML | Calcula similitud item-item y genera `models/item_sim.pkl` |
| `reco_service_stream.py` | Servicio ML en tiempo real | Consume ventas, calcula recomendaciones y las publica en Kafka/MongoDB |
| `print_recommendations_consumer.py` | Dashboard Recomendaciones | Muestra cada venta, productos comprados y sugerencias con mÃ©tricas de relevancia |
| `mongo_data_processor_completo.py` | Procesador | Crea agregaciones y anÃ¡lisis de datos |
| `erp_dashboard.py` | Dashboard EstÃ¡tico | VisualizaciÃ³n de datos procesados |
| `erp_dashboard_dinamico.py` | Dashboard DinÃ¡mico | Monitoreo en tiempo real |
| `config.py` | ConfiguraciÃ³n | ConfiguraciÃ³n centralizada del sistema |
| `docker-compose.yml` | Infraestructura | Servicios Docker (Kafka, Kafdrop, etc.) |

### ğŸ—„ï¸ Colecciones MongoDB

#### Datos Raw (Data Lake)
- `sales` - Transacciones de ventas
- `customers` - Base de datos de clientes
- `products` - CatÃ¡logo de productos
- `sales_items` - Detalle de items por venta

#### Datos Procesados (Data Warehouse)
- `daily_sales_summary` - ResÃºmenes diarios de ventas
- `store_analysis` - AnÃ¡lisis por sucursal
- `top_products` - Productos mÃ¡s vendidos
- `customer_analysis` - AnÃ¡lisis de clientes
- `category_analysis` - AnÃ¡lisis por categorÃ­as

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### ğŸ“‹ Prerrequisitos

- **Python 3.8+**
- **Docker y Docker Compose**
- **Git**
- **ConexiÃ³n a Internet** (para MongoDB Atlas)

### ğŸ”§ Dependencias Python

```bash
pip install pymongo confluent-kafka streamlit pandas plotly scipy seaborn matplotlib numpy kafka-python
```

O instalar desde requirements.txt:
```bash
pip install -r requirements.txt
```

### ğŸŒ ConfiguraciÃ³n MongoDB Atlas

1. **Crear cuenta en [MongoDB Atlas](https://cloud.mongodb.com/)**
2. **Crear cluster** (gratuito disponible)
3. **Crear usuario** con permisos de lectura/escritura
4. **Agregar IP a whitelist** (0.0.0.0/0 para desarrollo)
5. **Obtener connection string**

### âš™ï¸ ConfiguraciÃ³n del Sistema

1. **Actualizar `config.py`** con tus credenciales:
```python
MONGODB_USERNAME = "tu_usuario"
MONGODB_PASSWORD = "tu_contraseÃ±a"
MONGODB_CLUSTER = "tu_cluster.mongodb.net"
```

2. **Verificar puertos disponibles**:
   - Kafka: 9094
   - Kafdrop: 9000
   - Dashboard EstÃ¡tico: 8501
   - Dashboard DinÃ¡mico: 8502

## ğŸƒâ€â™‚ï¸ EjecuciÃ³n del Sistema

### 1ï¸âƒ£ Iniciar Infraestructura
```bash
# Iniciar servicios Docker (Kafka, Kafdrop, etc.)
docker-compose up -d

# Verificar que los servicios estÃ©n corriendo
docker ps
```

### 2ï¸âƒ£ Procesar Datos Reales
```bash
# Procesar datos reales del CSV Superstore (9,994 registros)
python process_superstore_csv.py
```

### 3ï¸âƒ£ Ejecutar Pipeline de Datos
```bash
# Terminal 1: Productor Kafka
python erp_simulator_producer.py

# Terminal 2: Consumidor Kafka â†’ MongoDB
python erp_kafka_to_mongo.py

# Terminal 3: Procesador de Datos
python mongo_data_processor_completo.py
```

### 4ï¸âƒ£ Ejecutar Dashboards
```bash
# Terminal 4: Dashboard EstÃ¡tico (Puerto 8501)
streamlit run erp_dashboard.py --server.port 8501

# Terminal 5: Dashboard DinÃ¡mico (Puerto 8502)
streamlit run erp_dashboard_dinamico.py --server.port 8502
```

## ğŸ“Š Dashboards

### ğŸ¯ Dashboard de Recomendaciones (Streamlit)
- **Archivo**: `print_recommendations_consumer.py`
- **Entrada**: tÃ³pico Kafka `erp.recommendations` o colecciÃ³n `recommendations` en MongoDB (respaldo)
- **QuÃ© muestra**:
  - InformaciÃ³n completa de la venta (cliente, tienda, pago, fecha, totales)
  - Productos comprados con nombres, categorÃ­as, cantidades, precios y subtotal
  - Productos recomendados con mÃ©tricas de ML en porcentaje (relevancia, confianza, ranking)
  - GrÃ¡ficos de barras e histogramas de relevancia, mÃ¡s mÃ©tricas agregadas
- **Extras**: botÃ³n de actualizaciÃ³n manual, auto-refresh cada 10 segundos, lectura de MongoDB cuando Kafka estÃ¡ vacÃ­o

### ğŸ“ˆ Dashboard EstÃ¡tico (Puerto 8501)
- **AnÃ¡lisis histÃ³rico** de ventas
- **Filtros por aÃ±o/mes**
- **MÃ©tricas agregadas** por sucursal
- **ExportaciÃ³n** a Excel/CSV
- **GrÃ¡ficos estadÃ­sticos**

### âš¡ Dashboard DinÃ¡mico (Puerto 8502)
- **Monitoreo en tiempo real**
- **Filtros por fecha**
- **Ventas por hora** (coloreadas por sucursal)
- **Auto-refresh** cada 5 segundos
- **Datos actualizados** constantemente

## ğŸ” Monitoreo y Debugging

### ğŸ“Š Kafdrop (Puerto 9000)
- **Monitoreo de Kafka** en tiempo real
- **VisualizaciÃ³n de tÃ³picos** y mensajes
- **MÃ©tricas de rendimiento**

### ğŸ³ Docker Logs
```bash
# Ver logs de Kafka
docker logs kafka

# Ver logs de MongoDB
docker logs mongo
```

### ğŸ“ Logs de AplicaciÃ³n
- **Productor**: Logs de envÃ­o de mensajes
- **Consumidor**: Logs de recepciÃ³n y guardado
- **Procesador**: Logs de agregaciones
- **Dashboards**: Logs de conexiÃ³n a MongoDB

## ğŸ“ˆ MÃ©tricas del Sistema

### ğŸ“Š Volumen de Datos (Datos Reales - Superstore)
- **Productos**: 1,862 productos Ãºnicos (con nombres, categorÃ­as y `unit_price` preservados en MongoDB)
- **Clientes**: 793 clientes Ãºnicos (nombres y correos generados para completar el dataset)
- **Ventas**: 5,009 Ã³rdenes
- **Items de Venta**: 9,994 lÃ­neas de venta
- **Recomendaciones ML**: hasta 5 sugerencias por venta con relevancia calculada por el modelo item-based

### âš¡ Rendimiento
- **Productor Kafka**: 20 mensajes/segundo
- **Consumidor**: Procesamiento en lotes de 1,000
- **Procesador**: Agregaciones optimizadas
- **Dashboards**: Cache de 5 segundos (dinÃ¡mico)

## ğŸ› ï¸ SoluciÃ³n de Problemas

### âŒ Error de ConexiÃ³n a MongoDB
```bash
# Verificar credenciales en config.py
# Verificar whitelist de IP en MongoDB Atlas
# Verificar que el cluster estÃ© activo
```

### âŒ Error de ConexiÃ³n a Kafka
```bash
# Verificar que Docker estÃ© corriendo
docker-compose up -d

# Verificar puerto 9094
netstat -an | findstr 9094
```

### âŒ Error de Dependencias
```bash
# Reinstalar dependencias
pip install --upgrade -r requirements.txt
```

## ğŸ“š Estructura de Archivos

```
Practica 4/
â”œâ”€â”€ ğŸ“ archive/                    # Dataset real (Sample - Superstore.csv)
â”œâ”€â”€ ğŸ“ csv_out/                    # Datos normalizados listos para Kafka
â”œâ”€â”€ ğŸ“ models/                     # Modelo de similitud (`item_sim.pkl`)
â”œâ”€â”€ ğŸ§  model_train_reco.py         # Entrenamiento del modelo de recomendaciones
â”œâ”€â”€ âš™ï¸ reco_service_stream.py      # Servicio en tiempo real (Kafka â†’ ML â†’ MongoDB)
â”œâ”€â”€ ğŸ¯ print_recommendations_consumer.py  # Dashboard de recomendaciones
â”œâ”€â”€ ğŸ”§ erp_simulator_producer.py   # Productor Kafka
â”œâ”€â”€ ğŸ”§ erp_kafka_to_mongo.py       # Consumidor Kafka â†’ MongoDB
â”œâ”€â”€ ğŸ”§ mongo_data_processor_completo.py  # Procesamiento de agregados
â”œâ”€â”€ ğŸ“Š erp_dashboard.py            # Dashboard estÃ¡tico
â”œâ”€â”€ âš¡ erp_dashboard_dinamico.py    # Dashboard dinÃ¡mico
â”œâ”€â”€ âš™ï¸ config.py                   # ConfiguraciÃ³n centralizada
â”œâ”€â”€ ğŸ³ docker-compose.yml          # Servicios Docker
â””â”€â”€ ğŸ“– README.md                   # Esta documentaciÃ³n
```

## ğŸ¯ Casos de Uso

### ğŸ“Š AnÃ¡lisis Comercial
- **Tendencias de ventas** por perÃ­odo
- **Rendimiento por sucursal**
- **Productos mÃ¡s vendidos**
- **AnÃ¡lisis de clientes**

### ğŸ” Monitoreo Operativo
- **Ventas en tiempo real**
- **Alertas de rendimiento**
- **MÃ©tricas de negocio**

### ğŸ“ˆ Reportes Ejecutivos
- **Dashboards interactivos**
- **ExportaciÃ³n de datos**
- **Visualizaciones avanzadas**

## ğŸš€ PrÃ³ximos Pasos

1. **Escalabilidad**: Implementar mÃ¡s nodos de Kafka
2. **Machine Learning**: Agregar algoritmos de recomendaciÃ³n
3. **Alertas**: Sistema de notificaciones en tiempo real
4. **API REST**: Exponer datos mediante API
5. **Microservicios**: Separar componentes en servicios independientes

---

## ğŸ‘¥ Autores

**Manuel Monegro** - Universidad - TecnologÃ­as Emergentes I

## ğŸ“„ Licencia

Este proyecto es parte de un trabajo acadÃ©mico para la materia de TecnologÃ­as Emergentes I.

---

*Sistema ERP de Big Data - Arquitectura de Recomendaciones Comerciales* ğŸ¢ğŸ“Š
